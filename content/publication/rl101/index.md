---
title: "Reinforcement Learning: All the Basics"
authors:
- admin
date: "2024-01-08"
doi: ""
publishDate: "2024-01-22"
share: false

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["Learnings"]

# Publication name and optional abbreviated publication name.
publication: "MohamedYosef101"
publication_short: ""

featured: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: true

# introduction
abstract: This is me summarizing the basics of Reinforcement Learning

# Summary. An optional shortened abstract.
summary: 

tags:
- Reinforcement Learning

links:
- name: "Medium"
  url: ""
- name: ""
  url: ""
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
---

## What is Reinforcement Learning?
A machine learning paradigm where anÂ **agent,** the AI, learns from theÂ **environment**Â byÂ interacting with it, through trial and error, andÂ receiving **rewards**, negative or positive, as a feedback for performing actions. Using this feedback, the agent can actively adapt to the environment. 


## The Reinforcement Learning Process
As you may guess, the RL process begins with the agent observing a situation or a state, $S_t$, from the environment (or the world) to get key information about the situation, the state; the agent is in.
1. Based on the state, $S_t$, the agentÂ selects an action, $A_t$,Â according to its current mindset, policy $\pi$.
2. The agent executes the action in the environment.Â This changes the situation, the state of the environment.
3. The environmentÂ provides a reward,Â $R_t$  (a numerical value), to the agent based on the consequences of its action.Â 
4. The agentÂ updates its policyÂ $\pi$ based on this feedback to favor actions the produce higher rewards.
5. TheÂ new stateÂ of the environment is observed, and theÂ process repeated.

<figure>
  <img alt="RL Process" src="https://miro.medium.com/v2/resize:fit:700/1*DJLDOROwMqoL9KUKsvc8Fg.png">
  <figcaption><b>Source:</b> <a href="https://mitpress.mit.edu/9780262352703/reinforcement-learning/">Reinforcement learning: An introduction</a> book by Richard S. Sutton and Andrew G. Barto</figcaption>
</figure>

<p style="color: crimson; font-weight: bold;">ğŸ¯ The agentâ€™s goal is toÂ maximizeÂ itsÂ expected returnÂ (cumulative reward).</p>


## Reinforcement Learning Terminology
TheÂ information the agent gets from the environment. In case of video game, each frame on the screen is like aÂ state,Â *s,*Â giving the agent information about the world (environment). In other cases, such as with a trading agent, theÂ stateÂ can represent the value of a specific stock, among other possibilities.
AnÂ observationÂ *o*Â is a partial description of a state. Sometimes, the agent has full visibility of the environment (**fully observed**), while other times, it only sees a partial picture (**partially observed**).

### Making choices: Action Spaces
**Action**Â is theÂ move, orÂ decisionÂ made by the agent in a given state of the environment. And theÂ **action space**Â is the set ofÂ all valid moves/actionsÂ in a given environment. Think of theÂ action spaceÂ as the agentâ€™s toolbox. In a game, it might have a set number of moves (**discrete action space**), like jumping or attacking. In continuous environments, the agent might control a robotâ€™s movement with precise values (**continuous action spaces**).

### Deciding what to do: Policies

TheÂ **policy**Â is the agentâ€™s brain, deciding what actions to take based on the observed state. Policy can be deterministic or stochastic;

- **Deterministic Policy:** a logical approach where the agent has a specific action to take for each stateÂ ***s*** and denoted by $\mu \rightarrow a_t = \mu_\theta(S_t)$. Itâ€™s like having a rule that says, â€œIf X happens, do Yâ€, with no exceptions. The agent follows the same rule every time itâ€™s in the same situation or state.

- **Stochastic Policy:** more flexible. Instead of one action, the agent has a set of actions and chooses one based on probabilities. Here, policy denoted by $\pi \rightarrow a_t \sim \pi_\theta(\cdot|S_t)$. <p style="color: #656565">Itâ€™sÂ like flipping a coin to decide what to do. The coin and the flipping have some randomness. You donâ€™t know that youâ€™ll get tail at the first flip and head in the second flip. But what you know is that if you flip the coin many times, youâ€™ll get 50% heads and 50% tails.</p>


### The feedback Loop: Rewards, Return & Discounting

The environment, our agentâ€™s world, providesÂ **rewards**Â to guide the agent after taking the action. Idea came from points in games; in football, the team gets 3 points for winning and 1 point for a draw and 0 points for losing. In our case, the agent gets the reward based on the current and future state and, of course, the action.

$$
r_t=R(S_t, A_t, S_{t+1})
$$

Instead of individual rewards, we often consider theÂ **return,**Â which sums up all future rewards. TheÂ **discount rate,** gamma $\gamma$, determines how much future rewards matter. AÂ **higher gamma**Â prioritizes long-term rewards (to take $100 after a year), while aÂ **lower gamma**Â focuses on immediate rewards (to take $20 now).

**Kinds of return:**

**1- Finite-horizon undiscounted return**, which is just the sum of rewards obtained in a fixed window of steps: $R(\tau)= \sum_{t=0}^{T} r_t$; where tau $\tau$ is the trajectory.

**2- Infinite-horizon discounted return**, which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future theyâ€™re obtained. 

$$
R(\tau)=\sum_{t=0}^{\infty} \gamma^t r_t
$$

> **Important definition**
Horizon: number of time steps in each episode; episode: is the agentâ€™s journey from a clear start to a specific end.
> 

### **Putting it all together: Trajectories**

**Sequence of states, actions, and rewards**Â the agent experiences in the world. Itâ€™s like playing a game, making choices, and seeing the outcomes â€” thatâ€™s a trajectory!

$$
\tau = (S_0, A_0, S_1, A_1, ...)
$$

## Types of Reinforcement Learning tasks

A task is a specificÂ **instance**Â of a problem. There are mainly two categories of tasks:Â episodicÂ andÂ continuous. **Episodic** tasks have a clear beginning and specific end, or a terminal state. In contrast, **continuous** tasks are ongoing, lacking a definitive endpoint, which requires the agent to improve the policy continuously while interacting with the environment.

![Image byÂ [MohamedYosef101](https://linkedin.com/in/mohamedyosef101)Â (the author)](https://miro.medium.com/v2/resize:fit:700/1*pCXjbasrMKINOnbXqetUSw.png)

Image byÂ [MohamedYosef101](https://linkedin.com/in/mohamedyosef101)Â (the author)

## **The Exploration/Exploitation trade-off**

Sometimes, agents need to explore to learn new things and exploit to use what they know to do well. But the question is still: how to balance between exploration and exploitation?

- **Exploration**: when the agent tries out different things in the environment to learn more about it. Itâ€™s like looking around to find new information.
- **Exploitation**: when the agent uses what it already knows to get the best results. Itâ€™s like using a map youâ€™ve made to find the quickest route to a treasure.

![**Source**:Â [Exploration-Exploitation Dilemma (opengenus.org)](https://iq.opengenus.org/exploration-exploitation-dilemma/)](https://miro.medium.com/v2/resize:fit:462/0*5xpDJV7cK7CFK767.png)

**Source**:Â [Exploration-Exploitation Dilemma (opengenus.org)](https://iq.opengenus.org/exploration-exploitation-dilemma/)

## Two approaches for solving RL problems

The PolicyÂ is the function we want to learn. Our goal is to find theÂ optimalÂ policy Ï€*, the policy that maximizes expected return when the agent acts according to it. We find thisÂ Ï€* through training.

There are approaches to find this optimal policy *Ï€**:

- **Directly**, by teaching the agent to learn which action to take, given a state; policy-based methods.
- **Indirectly**, teach the agent to learn which state is more valuable and then take the action that leads to more valuable states; value-based methods.