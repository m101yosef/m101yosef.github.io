---
title: "ML Mathematics (SOON)"
summary: "Master the esential math behind ML models"
date: 2025-07-30
type: docs
weight: 3
---

Welcome to the course! This course covers the basics of the topic and practical examples. Each lecture will guide you through a specific topic with explanations and code samples.


## Course Outline 


### Linear Algebra basics
[Lecture 1](#). Vectors - definition, addition, scalar multiplication <br>
[Lecture 2](#). Matrices - operations, types, shapes <br>
[Lecture 3](#). Matrix multiplication (dot product + rules) <br>
[Lecture 4](#). Transpose, identity, and inverse <br>
[Lecture 5](#). Linear systems and solving with matrices <br>

### Linear Algebra for ML
[Lecture 6](#). Vector spaces and span <br>
[Lecture 7](#). Linear independence and rank <br>
[Lecture 8](#). Projections and geometry of data <br>
[Lecture 9](#). Eigenvalues and Eigenvectors (with visuals) <br>
[Lecture 10](#). Principal component analysis (PCA) <br>

### Calculus basics
[Lecture 11](#). Functions, limits and derivatives <br>
[Lecture 12](#). Rules of differentiation (product, chain rule) <br>
[Lecture 13](#). Partial derivatives (multivariable functions) <br>
[Lecture 14](#). Gradients, level curves, and contours <br>
[Lecture 15](#). Optimisation with gradient descent <br>

### Calculus for ML
[Lecture 16](#). Taylor series and approximaitons <br>
[Lecture 17](#). Jacobian and Hessian matrices <br>
[Lecture 18](#). Convex functions and optimisation <br>
[Lecture 19](#). Backpropagation (deep dive math) <br>
[Lecture 20](#). Autograd in PyTorch <br>


### Probability and distributions
[Lecture 21](#). Probability rules, independence, Bayes' rule <br>
[Lecture 22](#). Random variables, expectation, variance <br>
[Lecture 23](#). Common distributions (Bernoulli, Binomial, Normal) <br>
[Lecture 24](#). Multivariate distributions and covariance <br>
[Lecture 25](#). Maximum Likelihood Estimation (MLE) <br>

### Information theory & ML math tools
[Lecture 26](#). Entropy, KL divergence, cross-entropy <br>
[Lecture 27](#). Information gain and decision trees <br>
[Lecture 28](#). Linear regression <br>
[Lecture 29](#). Logistic regression (sigmoid + cross-entropy) <br>
[Lecture 30](#). Regularisation (L1, L2), bias-variance tradeoff <br>

