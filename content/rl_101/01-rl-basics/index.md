---
title: The fundamentals of reinforcement learning explained.
date: '2024-01-22'
summary: Learn the basics of the rising star in the world of AI; reinforcement learning.

share: false
featured: true

authors:
  - admin

tags:
  - Reinforcement Learning

categories:
  - RL 101
---

# Intro

Reinforcement learning is theÂ **rising star**Â in the world of AI that is impossible to ignore. While other techniques like supervised learning have shed light on narrow domains, reinforcement learning beams with promise to trulyÂ **revolutionize how machines learn andÂ [make decisions](https://medium.com/@mohamedyosef101/markov-decision-processes-given-a-model-of-the-world-761fc4147cbf).**

While DeepMindâ€™sÂ **[AlphaGo](https://youtu.be/WXuK6gekU1Y?si=0MSwFFFaEawDc6dY)**Â defeating the world Go champion in 2016 remains a landmark achievement, the field of reinforcement learning has progressed even further in recent years. Beyond mastering complex games likeÂ **Dota 2**Â with OpenAIâ€™s bot, reinforcement learning algorithms areÂ **now**Â makingÂ **significant strides in real-world applications**.


# **So, what is Reinforcement learning?**

A machine learning paradigm where anÂ **agent**Â (the AI) learn form theÂ **environment**Â byÂ **interacting with it**Â (through trial and error) andÂ **receiving rewards**Â (negative or positive) as a feedback for performing actions.

# **The RL process**

![](https://miro.medium.com/v2/resize:fit:700/1*DJLDOROwMqoL9KUKsvc8Fg.png)

**Source**:Â *[Reinforcement learning: An introduction](https://mitpress.mit.edu/9780262352703/reinforcement-learning/)Â book by*Â Richard S. Sutton and Andrew G. Barto.

AgentÂ **observes the current state**Â of the environmentÂ *(to get key information about the situation the agent is in).*

1. Based on the state, the agentÂ **selects an action**Â according to its current policy.
2. The agent executes the action in the environment.Â *This changes the state of the environment.*
3. The environmentÂ **provides a reward**Â (a numerical value) to the agent based on the consequences of its action.Â *Positive rewards encourage the agent and negative rewards discourage it.*
4. The agentÂ **updates its policy**Â based on this feedback to favor actions the produce higher rewards.Â *This is done by techniques likeÂ [Q-learning](https://medium.com/@mohamedyosef101/how-q-learning-works-in-reinforcement-learning-6d85e0cb6668),Â [policy gradient](https://medium.com/@mohamedyosef101/reinforcement-learning-policy-gradient-101-de2a48f87e5b), etc.*
5. TheÂ **new state**Â of the environment is observed and theÂ **process repeated**â€¦

> ğŸ¾ The agentâ€™s goal is toÂ maximizeÂ itsÂ expected returnÂ (cumulative reward).
> 

# **Reinforcement Learning Terminology**

!https://miro.medium.com/v2/resize:fit:700/1*VRcT4KHWjGG8zi5t6HAaqQ.png

Image byÂ [MohamedYosef101](https://linkedin.com/in/mohamedyosef101)Â (the author)

## **What does the agent see? Observations/States**

TheÂ **information the agent gets from the environment**. In case of video game, each frame on the screen is like aÂ **state**Â *s,*Â giving the agent information about the world (environment). In other cases, such as with a trading agent, theÂ **state**Â can represent the value of a specific stock, among other possibilities.

AnÂ **observation**Â *o*Â is a partial description of a state. Sometimes, the agent has full visibility into environment (***fully observed***), while other times, it only sees a partial picture (***partially observed***).

â€”

## **Making choices: Action Spaces**

**Action**Â is theÂ **move**Â orÂ **decision**Â made by the agent in a given state of the environment. And theÂ **action space**Â are the set ofÂ **all valid moves/actions**Â in a given environment.

Think of theÂ **action space**Â as the agentâ€™s toolbox. In a game, it might have a set number of moves (**discrete action space**), like jumping or attacking. In continuous environments, the agent might control a robotâ€™s movement with precise values (**continuous action spaces**).

â€”

## **Deciding what to do: Policies**

TheÂ **policy**Â is the agentâ€™s brain, deciding what actions to take based on the observed state.

- It can beÂ **[deterministic](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#deterministic-policies)Â (***fixed action for each state***)**Â and denoted byâ€¦

https://miro.medium.com/v2/resize:fit:274/0*PquoXM-NKMiO9c3S

- Or it may beÂ **[stochastic](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies)Â (***probability-based***),**Â like flipping a coin to choose, and denoted byâ€¦

https://miro.medium.com/v2/resize:fit:298/0*3YrkFdAOVo_SRIeK

> Remember:â€œpolicyâ€Â and â€œagentâ€ are often used interchangeably.
> 

â€”

## **The feedback Loop: Rewards, Return & Discounting**

The environment (world) providesÂ **rewards**Â to guide the agent after taking the action. Think of positive points in a game or praise for a good robot behavior.

https://miro.medium.com/v2/resize:fit:315/0*xou8tvahfUJGL2Mb

Instead of individual rewards, we often consider theÂ **return,**Â which sums up all future rewards. TheÂ **discount rate**Â (gamma) determines how much future rewards matter. AÂ **higher gamma**Â prioritizes long-term rewards, while aÂ **lower gamma**Â focuses on immediate rewards.

**Kinds of return:**

**1- Finite-horizon undiscounted return**, which is just the sum of rewards obtained in a fixed window of steps:

https://miro.medium.com/v2/resize:fit:231/0*L0yo7_F01w8T_Tp8

**2- Infinite-horizon discounted return**, which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future theyâ€™re obtained.

https://miro.medium.com/v2/resize:fit:267/0*J_GWyA88CfpEfCMs

â€”

## **Putting it all together: Trajectories**

A trajectory is aÂ **sequence of states, actions, and rewards**Â the agent experiences in the world. Itâ€™s like playing a game, making choices, and seeing the outcomes â€” thatâ€™s a trajectory!

# **Types of RL tasks**

A task is anÂ **instance**Â of a problem. In RL, we primarily have two types of tasks;Â **episodic**Â andÂ **continuous**.

!https://miro.medium.com/v2/resize:fit:700/1*pCXjbasrMKINOnbXqetUSw.png

Image byÂ [MohamedYosef101](https://linkedin.com/in/mohamedyosef101)Â (the author)

## **Episodic task**

In this case, we have aÂ **starting**Â point and anÂ **ending point**Â (**a terminal state**). This creates an episode:Â *a list of States, Actions, Rewards, and new States.*

## **Continuing tasks**

Tasks thatÂ **continue forever**Â (**no terminal state**).Â *In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.*

# **ğŸ› Â The Exploration/Exploitation trade-off**

- **Exploration**Â isÂ *exploring the environment*Â byÂ **trying random actions to find information**Â about the environment.
- **Exploitation**Â is usingÂ **known**Â information toÂ **maximize**Â the reward.

!https://miro.medium.com/v2/resize:fit:462/0*5xpDJV7cK7CFK767.png

**Source**:Â [Exploration-Exploitation Dilemma (opengenus.org)](https://iq.opengenus.org/exploration-exploitation-dilemma/)

# **Two approaches for solving RL problems**

In other words, how can we build the RL agent canÂ **select the actions that maximize its expected return**Â (cumulative reward).

**The Policy: the agentâ€™s brain**

> The PolicyÂ is the function we want to learn, our goal is to find theÂ optimalÂ policy Ï€*, the policy that maximizes expected return when the agent acts according to it. We find thisÂ Ï€* through training.
> 

***There are approaches to find this optimal policy*Â Ï€**:***

- **Directly**,Â *by teaching the agent to learn which action to take, given the current state:*Â **[Policy-Based Methods](https://medium.com/@mohamedyosef101/the-fundamentals-of-reinforcement-learning-explained-f42de0053fc7#0158).**
- **Indirectly**,Â *teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states:*Â **[Value-Based Methods](https://medium.com/@mohamedyosef101/the-fundamentals-of-reinforcement-learning-explained-f42de0053fc7#4b00)**.

# **1. Policy-based methods**

This function will define a mapping from each state to the best corresponding action. Alternatively, it could defineÂ **a probability distribution over the set of possible actions at that state.**

**ğŸ›  We have two types of policies:**

- **[Deterministic](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#deterministic-policies)**: a policy at a given state will always return the same action.

https://miro.medium.com/v2/resize:fit:173/0*lm3iBdCqNMIRmMMT

- **[Stochastic](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies)**: outputs a probability distribution over actions.

https://miro.medium.com/v2/resize:fit:303/0*Z8pYYEUDwE2DWn1-

__________________

>>Â **Additional Reading:**

- [Model-Free Policy Learning](https://medium.com/@mohamedyosef101/model-free-policy-learning-08d163338604)
- [Policy Gradient](https://medium.com/@mohamedyosef101/reinforcement-learning-policy-gradient-101-de2a48f87e5b)

__________________

# **2. Value-based methods**

The value of a state is theÂ **expected discounted return**Â the agent can get if it starts in that state, and then acts according to our policy.

> â€œAct according to our policyâ€ just means that our policy isÂ â€œgoing to the state with the highest valueâ€.
> 

![](https://miro.medium.com/v2/resize:fit:700/0*-lAh7eNJBR5YVOns.jpg)

***Source**:Â [Two main approaches for solving RL problems (huggingface.co)](https://huggingface.co/learn/deep-rl-course/unit1/two-methods)*

# Now letâ€™s see value-based methods in a closer lookâ€¦

**The two primary types of value-based methods are:**

## **a. State-value methods â€œV(s)â€**

Estimate the value of being in a particular state.

**Value function:**Â V(s) represents the expected long-term reward starting from state s, following the current policy.

**Common algorithms:**

- [Monte Carlo](https://medium.com/@mohamedyosef101/model-free-policy-learning-08d163338604#4911)Â evaluation
- [Temporal Difference](https://medium.com/@mohamedyosef101/model-free-policy-learning-08d163338604#50ca)Â (TD) learning, including SARSA and Q-learning

## **b. Action-value methods â€œQ(s, a)â€**

Estimate the value of taking a specific action in a given state.

**Value function:**Â Q(s, a) represents the expected long-term reward starting from state s, taking action a, and then following the current policy.

**Common algorithms:**

- [Q-learning](https://medium.com/@mohamedyosef101/how-q-learning-works-in-reinforcement-learning-6d85e0cb6668)
- [Deep Q-Networks](https://medium.com/@mohamedyosef101/deep-q-learning-reinforcement-learning-plus-neural-network-c40ce32d034b)Â (DQN)

!https://miro.medium.com/v2/resize:fit:700/1*gtxTOWsx1cLS2r0paQqTCA.png

**Source**:Â [Two types of value-based methods (huggingface.co)](https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods)

# **Outro**

This introduction may have left you with more questions than answers. Thatâ€™s perfectly normal! Reinforcement learning is a complex and ever-evolving field, and the best way to truly understand it is to get your hands dirty.Â **Donâ€™t be afraid to experiment**, make mistakes, and learn from them.

## **Up Next â€¦**

**[Markov Decision Processes](https://medium.com/@mohamedyosef101/markov-decision-processes-given-a-model-of-the-world-761fc4147cbf)**;Â *for now think of it asÂ [Tetris](https://en.wikipedia.org/wiki/Tetris), where you choose the best move in uncertain future based on rewards & past experience.*

## **References**

- Thomas Simonini. (2018).Â *[Deep Reinforcement Learning course](https://huggingface.co/learn/deep-rl-course/unit0/introduction).*Â huggingface.
- Emma Brunskill. (2019).Â *[CS234: Reinforcement Learning](https://youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&si=iDTVkCL63ER_LWvm).*Â Stanford Online.
- Richard Sutton & Andrew Barto. (2018, 2020).Â *[Reinforcement learning: An introduction](https://mitpress.mit.edu/9780262352703/reinforcement-learning/)*. MIT Press.

# **â€¦ ØªÙ… Ø¨Ø­Ù…Ø¯ Ø§Ù„Ù„Ù‡ â€¦**

<div><br></div>

## Read the [full article](https://medium.com/@mohamedyosef101/the-fundamentals-of-reinforcement-learning-explained-f42de0053fc7) on Medium
